{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "550fd25e-7caa-4eca-9cc7-4ad0f8cdb826",
   "metadata": {},
   "source": [
    "# Scatter Plots, Data Fitting, and Histograms\n",
    "\n",
    "We are going to load in a different dataset, located in data_linfit.txt. This is a more basic file that is just separated by spaces with no headers but we can read it in just fine with `genfromtxt`\n",
    "\n",
    "We will make a scatter plot using this data with matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566b4f67-3b44-434b-8f86-27d042c04186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#note that genfromtxt autmoatically knows that spaces separate the data (that's the default)\n",
    "x, y = np.genfromtxt('data_linfit.txt',unpack=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d4f4fa-e94c-4518-bf48-a7c0b51e6c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#scatter takes x, y arguments. You can also set the marker.\n",
    "#Colors can be set too, just like in plot()\n",
    "plt.scatter(x,y, marker='x', color='k')\n",
    "plt.ylabel('y',fontsize='large')\n",
    "plt.xlabel('x',fontsize='large')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d30440-c72b-4a73-854f-4bec51a8f1ea",
   "metadata": {},
   "source": [
    "## Linear fits with Scipy\n",
    "\n",
    "As we often want to do in science, we will fit this data to a line using linear regression.\n",
    "\n",
    "The toolkit Scipy has a lot of useful statistics tools. This linear regression function, located in the stats package, outputs an object that inclues all the fit values and their associated statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0835dd07-ee59-4d3f-b52f-42d00428ac4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy # scipy is another package we need\n",
    "my_linear_fit = scipy.stats.linregress(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f2420b-3e6a-4bf1-922b-41f8a80dc6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_linear_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a0ef4d-ebec-4369-8b58-1a6ca8fe82d0",
   "metadata": {},
   "source": [
    "The slope and intercept give you your line. The rvalue can be squared to give the r^2 value (goodness of fit). The p-value is another statistic that essentially tells you how likely it is that there is no correlation between these quantities (very small here!). Then theres errors on the slope (stderr) and intercept (intercept_stderr).\n",
    "\n",
    "The way we can access this data is by asking for these properties of the linear fit *object* the function returns. This object has all of these things as properties that we can get by typing things like `my_linear_fit.intercept`, etc.\n",
    "\n",
    "We can now use this to plot our fit against our data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb7123a-d614-4eb0-9070-760a5172fe85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_fit(x):\n",
    "    return my_linear_fit.slope*x + my_linear_fit.intercept\n",
    "\n",
    "plt.scatter(x,y, marker='x',color='k')\n",
    "plt.plot(np.linspace(0,100,100),my_fit(np.linspace(0,100,100)), color='r', linewidth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fec2c7c-2bd2-4a11-8456-2660124a4e53",
   "metadata": {},
   "source": [
    "Now we can check our fit. If it is good then there should be roughly an even distribution of points above and below the line. Let's see if that's true by checking the *residuals* of the data (the difference between the actual and predicted values, treating x as the independent variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e487b39b-c89f-46f9-afe6-b37d7272b34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dy = y - my_fit(x) #our residuals\n",
    "#let's just pring out the number of positive vs negative residuals\n",
    "print(len(dy[(dy>0)]), len(dy[(dy<0)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf89748a-de82-4059-808c-247f450c7bda",
   "metadata": {},
   "source": [
    "Not too bad! Very close! We can also take a closer look at the distribution of residuals, just for fun. We will use the hist() function in matplotlib which produces a histogram (binned numbers). It takes as an argument one set of data, as well as a number of bins to split it up into. You may also give it a `range=(min,max)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3a64d0-42d4-489c-94de-1a8c475265d2",
   "metadata": {},
   "outputs": [],
   "source": [
    " # you can play around with the number of bins. What happens if you make them too small?\n",
    "plt.hist(dy, bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc1dda8-dfda-47fe-82c2-8331ec76dfc9",
   "metadata": {},
   "source": [
    "For your lab reports, it will always be important to report the goodness of any fit you perform. In the case of linear regression, this is the rvalue^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6718cb7-d0ba-43f2-a706-9eda5bdfcd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('r-squared value:', my_linear_fit.rvalue**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf81ca9-0087-4789-b5d9-5408aa950fca",
   "metadata": {},
   "source": [
    "That's very close to one! That's very good!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a40ab3-a5a8-4650-ac2b-7e32c38166cd",
   "metadata": {},
   "source": [
    "Let's see what happens when we fit random data (i.e. trying to find a correlation when there is none). In this case, we are going to assign 100 random numbers some other independent variable. This variable isn't random but just increasing, but this doesn't matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f682986a-4248-41f4-beed-305cee0c7859",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_num = np.random.rand(100) #100 random numbers\n",
    "ind_var = np.arange(100) #0,1,2...99\n",
    "my_new_linear_fit = scipy.stats.linregress(ind_var, rand_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ceea35-20e0-47a3-a465-76c35eb3cf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_new_linear_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320e6876-f901-4f7f-86f9-c74c771777c8",
   "metadata": {},
   "source": [
    "Wow. So that slope is very close to zero, and that pvalue is now pretty darn high! This means that it is likely there is no correlation (which of course there isn't!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be51f619-72d8-4940-9d3e-8db8fd2ef52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(ind_var, rand_num, marker='x', color='k')\n",
    "def my_new_fit(x):\n",
    "    return my_new_linear_fit.slope*x + my_new_linear_fit.intercept\n",
    "\n",
    "plt.plot(np.linspace(0,100,100),my_new_fit(np.linspace(0,1,100)),color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9804b9-8f09-46d2-8112-ed373381d185",
   "metadata": {},
   "source": [
    "Now let's take a look at the residuals again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beadb19-0780-4929-b2eb-96b107d091e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's look at the residuals again\n",
    "dy_new = rand_num - my_new_fit(x) #our residuals\n",
    "#let's just pring out the number of positive vs negative residuals\n",
    "print(len(dy_new[(dy_new>0)]), len(dy_new[(dy_new<0)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec3f857-cfab-4de6-a55e-894e2825a633",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dy_new, bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83716b0-2c2f-4d6f-8cdb-9c1b8ca32bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And finally the r^2 value\n",
    "\n",
    "print(my_new_linear_fit.rvalue**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7f132a-52c0-44f2-8321-dfcbf9ff48d5",
   "metadata": {},
   "source": [
    "Wow that's very small! This isn't a good fit... the data is random so that is what we'd expect!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83459792-dad5-43f9-955a-09f2018f13c8",
   "metadata": {},
   "source": [
    "## Error Bars\n",
    "\n",
    "If your data has some error bars associated with it you can plot those too using `plt.errorbar(x, y, xerr=dx, yerr=dy)`.\n",
    "\n",
    "We will typically assume that the errors/uncertainties  in data are *symmetric* and *gaussian*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1980201-f679-454d-9f99-fcdd1d655636",
   "metadata": {},
   "outputs": [],
   "source": [
    "xdata = [10,13.3, 16.2, 33.4]\n",
    "ydata = [2, 13, 18.9, 20.2,]\n",
    "xerror = [0.5, 0.3, 4, 5]\n",
    "yerror = [2, 0.5, 6, 10]\n",
    "#by default this will plot a line. If you want a scatter plot set linestyle=''\n",
    "plt.errorbar(xdata, ydata, xerr=xerror, yerr=yerror, linestyle='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2e0fcd-3df1-4809-a798-f0e90902949e",
   "metadata": {},
   "source": [
    "# Mean, Standard Deviation, and Gaussian Distributions\n",
    "\n",
    "The *Gaussian* distribution is crucial for error analysis. It is an exmple of a *probability distribution* representing the likelihood that a value devaites from some mean or predicted value. When you state that your error is +/- some number, the implication is that you are saying something about how wide (uncertain) or narrow (very certain) this distribution is. For small errors, you are saying that it is very *unlikely* that the true value deviates significantly from your measured result. When you see plots with error bars like the one above, it is very often the case that the errors are meant tO represent something like a Gaussian distribution of possible true answers relative to the measured result presented (\"we measure this value but we cannot really rule out anything in this region\")\n",
    "\n",
    "The *Gaussian* or *Normal* distribution has the following mathematical form $f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}$\n",
    "\n",
    "Where $\\mu$ is the mean or measured value and $\\sigma$ is the *standard deviation*, represending how wide or narrow the distribution is. This $\\sigma$ is what is typically showed as error bars in plots, or the uncertainty associated with a measurement. \"This region is within one standard deviation away from the measured/mean value\".\n",
    "\n",
    "## But what is a probability distribution?\n",
    "\n",
    "Given a probability distribution function $f(x)$, $f(x)dx$ represents the probability that x is between x and x+dx. More generally we can write probability that x is between value $a$ and $b$ as\n",
    "\n",
    "$\\int_a^b f(x)dx$\n",
    "\n",
    "The probability of something being true has to be between 0 (never true) and 1 (always true). So every probability distribution is *normalized* such that the integral over the entire allowed range of $x$ is 1 (the true values must be *somewhere*). So, for a value $x$ that can be any real number:\n",
    "\n",
    "$\\int_{-\\infty}^{\\infty} f(x)dx = 1$\n",
    "\n",
    "Below I define a function that will give the value of a Gaussian probability distribution for a given mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d9bad8-d0ee-4efa-9eb4-ee0891915de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(x, sigma=10, mean=0):\n",
    "    return 1/(sigma*np.sqrt(2*np.pi)) * np.exp(-0.5*((x-mean)/sigma)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb7ecea-d8e3-4947-9e68-36d445ef0c81",
   "metadata": {},
   "source": [
    "Numpy has a useful built-in function for sampling a normal (gaussian) distribution, called `np.random.normal()`.\n",
    "\n",
    "Below we will sample this normal distribution for a fixed number of times, starting with a mean of 0 and a $\\sigma=10$ (but you can set these to whatever you want!)\n",
    "\n",
    "Note that below in the `plt.hist()` function we have the keyword `density=True`. This *normalizes* the distribution so that the integral is 1. In other words, it turns a binned number count (regular histogram) into a *probability distribution* so that we can compare it to a Normal (Guassian) distribution. It does this by dividing each individual bin number count by the total number of samples (`N_sample`) and the size of the given bin range.\n",
    "\n",
    "In a lot of science, normalizing histograms like this is very useful because it makes the values more generally meaningful, rather than dependent on the number of samples you have. The important thing isn't the raw number of counts, but relative difference between counts at different values. By normalizing like this, one could compare different distributions that have different sample numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0cbca2-e7d7-47bd-8a4d-14a18c6bda7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_sample = 100\n",
    "sigma=30\n",
    "mean=30\n",
    "\n",
    "rand_normal  = np.random.normal(loc=mean, size=N_sample, scale=sigma)\n",
    "\n",
    "plt.hist(rand_normal, bins=20, density=True)\n",
    "plt.plot(np.linspace(mean-(5*sigma),mean+(5*sigma),1000), \n",
    "         gaussian(np.linspace(mean-(5*sigma),mean+(5*sigma),1000), sigma=sigma, mean=mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6b2dc4-898e-479e-a283-495554a9073d",
   "metadata": {},
   "source": [
    "Try the above for different means and sigmas and see what it looks like. It actually doesn't look great does it?\n",
    "\n",
    "One thing to consider is that the standard deviation ($\\sigma$) has an actual meaning when it comes to probabilities. One standard deviation on either side of the mean (i.e. the range $(\\mu-\\sigma, \\mu+\\sigma)$ has a probability of about 0.68 (or 68\\%). \n",
    "\n",
    "Let's see what it is for our distribution. Note that I use the function `np.abs()` which takes the absolute value, rather than writing the range like I did above. It is just shorter!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15149589-e0cf-40cf-8933-b9be41eb798b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rand_normal[(np.abs(rand_normal-mean)<sigma)])/N_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799e1f6e-b34e-4e3b-b9dd-2de0b54741b3",
   "metadata": {},
   "source": [
    "Now, this is random, so I don't know exactly what you'll get. But if you try it a few times with `N_sample =100` you'll get some variation of this fraction. \n",
    "\n",
    "**Now, try to increase `N_sample` to values like 1000 and 10000. What happens to your plot and to your fraction of numbers within one standard deviation?**\n",
    "\n",
    "If one sigma means 67% likelihood, what happens if we go out to $2\\sigma$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc83f3b-33dd-4625-94b7-9cc7785b43d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rand_normal[(np.abs(rand_normal-mean)<2*sigma)])/N_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee0018a-21db-4686-b05c-966a747e4f4c",
   "metadata": {},
   "source": [
    "And now $3\\sigma$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e962d63-25e1-4e4f-b73e-d7402b55ea3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rand_normal[(np.abs(rand_normal-mean)<3*sigma)])/N_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215449d8-fdaa-494c-b873-08bd87a0c9df",
   "metadata": {},
   "source": [
    "You should see that $2\\sigma$ encompasses roughly 95.5\\% of the points and $3\\sigma$ about 99.7\\%. When you are thinking of your data and the meaning behind your error bars, keep this in mind. Usually errors are expressed in terms of $\\sigma$. So \"one-sigma\", or the area encompassed by an error bar, represents a likelihood of about 67% that the \"true\" answer is within that range. and $3\\sigma$ is 99\\%. So, it isn't wild to think a number might lie outside of the one-sigma region... but three-sigma? Now there should only be about a 3 in 1000 chance that the truth lies outside of this region."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedfd233-0b6b-4ddf-9d84-9239bc2a781a",
   "metadata": {},
   "source": [
    "# Counting Errors\n",
    "\n",
    "Counting Error/Shot Noise/Poisson Noise is how to (approximately) quantify the uncertainty in sampling the true likelihood of an event. Given a true underlying probabiliy of an event occuring, you might expect to see the event happen $M$ times out of $K$ tries. This quantifies the likelihood of instead detecting $N$ events.\n",
    "\n",
    "This likelihood is governed by the Poison distribution. We won't worry about the functional form here, though for large samples it approaches a Gaussian distribution. The uncertainty (or \"shot noise\") in an observed count of $N$ events is $\\sqrt{N}$. Should I observe $N$ events, I would have had a reasonable chance of having observed $N\\pm\\sqrt{N}$ events.\n",
    "\n",
    "If we think about signal-to-noise - number of detections (signal) divided by this \"shot noise\" we get $\\frac{N}{\\sqrt{N}} = \\sqrt{N}$. So as the number of detections increases we become more and more condident that we are sampling the true probability.\n",
    "\n",
    "You will often see such errors when trying to measure a probability (fraction of events that meat some criteria given a sample of N total events). If the number of events that meat your criteria (say the number of heads from a coin flip) is $M$ and the number of total events observed is  $N$ than your measured probability is $M/N$ with an uncertainty of $\\frac{\\sqrt{M}}{N}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c77b30-588d-42db-bb2c-8ee744d76b3e",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "a) Use the `np.random.rand()` function to generate uniformly distributed random numbers between 0 and 1. Do this for a variety of different sample sizes spanning 10 to 1,000,000. For each sample, estimate the measured probability of finding a number less than 0.5. Plot this as a function of the number sampled and provide the appropriate errors. Do this as a scatter plot using `plt.errorbar()`. You will have to re-scale your axes for this! Use `plt.xscale('log')` to make your x-axis (number of samples) in log scale.\n",
    "\n",
    "b) Provided below is a code that simulates dice rolls of variable sided dice. Generate large numbers of dice rolls with different combinations of multiple dice (e.g. 1 d6 + 1 d8, 3 d6 + 1 d10, etc), summing their results. Plot the *probability distribution* of the values you find. Calculate the mean and standard deviation of your sample and compare your measured probability distribution to a gaussian using the function provided above. Make plots showing this comparison, with the measured mean and standard deviations shown by verticle lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828038cb-dfc7-468d-88e0-dcedc4a54d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_roll(n,d=6):\n",
    "    '''\n",
    "    Code by Mark Kennedy\n",
    "    Rolls a dice of face=d n times, and returns the results. \n",
    "    n : integer\n",
    "    d : integer\n",
    "    returns: array of results for n tosses.\n",
    "    '''\n",
    "    rng = np.random.default_rng()\n",
    "    rolls = np.floor(d*rng.random((n,)))+1 # Have to add one, as the function returns rolls from interval [0,d-1]\n",
    "    return rolls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
